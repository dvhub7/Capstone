---
Author: John-Paul
CKME 136: Capstone Fall 2017
Title: Sales_forecast
output:
  html_document: default
  pdf_document: default
---
  
##  Step 1. LOADING THE DATA
##  Beginning of the data preprocessing stage. 
##  Setting up a working directory and loading the data and libraries into R
```{r echo=TRUE}

# Setting working directory 
setwd('C:/Users/endcore/Desktop/CKME999/CKME136-DataAnalyticsCapstone/Dataset/')

## Load libraries and read the data files
library(tidyverse)
library(reshape2)
library(lubridate)
library(rpart)
library(rattle)
library(forecast)
library(tseries)
library(dygraphs)
library(fpp)

## Contains data with the following information
# Store Dept Date Weekly_Sales IsHoliday: 2010-02-05 ~ 2012-10-26
tr_df <- read_csv("train.csv")

## Contains data with the following information
# Store Dept Date IsHoliday: 2012-11-02 ~ 2013-07-26
tst_df <- read_csv("test.csv")

## Contains data with the following information
# Store Type Size
str_df <- read_csv("stores.csv")

## Contains data with the following nformation
# Store Date Temperature Fuel_Price Markdown1-5 CPI Unemplyment IsHoliday
ft_df <- read_csv("features.csv")


```
## Step 2: DATA PREPARATION FOR ANALYSIS
  Collecting and preparing the data for analysis are often the most involved and time consuming parts of building a         predictive model. While the collection was done for us, we still have to do a bit of work to prepare the data.
  Merge the data frames "train", "Feature" and store" using the variable "store". 
```{r echo=TRUE}

str(tr_df)
str(ft_df)
str(tst_df)
str(str_df)
head(tr_df)
head(tst_df)


# Analysis for train dataset and test dataset 
# Converting the  Store, Dept and Type features to factor datatype
str_df$Store <- factor(str_df$Store)
tr_df$Store <- factor(tr_df$Store)
tst_df$Store <- factor(tst_df$Store)
ft_df$Store <- factor(ft_df$Store)
tr_df$Dept <- factor(tr_df$Dept)
tst_df$Dept <- factor(tst_df$Dept)
str_df$Type <- factor(str_df$Type)

## Merge train dataset with stores dataset by store and features dataset 
# training dataset from the following dates: 2010-02-05 ~ 2012-10-26
train1 <- full_join(tr_df, str_df, by= "Store")
train <- merge(x=train1, y=ft_df, by=c("Store", "Date", "IsHoliday"), all.x=TRUE, sort=FALSE)

# Export the train datatset
#Create a csv file with the merged dataset train
 
#write.csv(train, file = "C:/Users/endcore/Documents/Capstone/train_merged.csv",row.names=FALSE, na="")

## Merge test dataset with stores dataset by store, Date and IsHoliday features
# testing dataset from the following dates: 2012-11-02 ~ 2013-07-26
test1 <- merge(tst_df, str_df, by="Store", sort = FALSE)
test <- merge(x=test1, y=ft_df, by=c("Store", "Date", "IsHoliday"), all.x=TRUE)
test <- arrange(test, Store, Dept)

rm(tr_df,tst_df,str_df,ft_df, test1, train1)

# Finding which column has NA values
list.NA<-""
for (i in c(1:ncol(train)))
{
  len<-length(grep("TRUE",is.na(train[,i])))
  if(len > 0){
    list.NA<-paste(colnames(train[i]),":",len,list.NA)
  }
}
glimpse(list.NA)
str(train)

# Number of missing values
sum(is.na(train))

levels(train$Type) # 3 retail store types: "A", "B", and "C"
levels(train$Store)

## We can then use the complete.cases function to identify the rows without missing data:

#train1 <- train[complete.cases(train),]
#test1 <- test[complete.cases(test),]

hist(log(train$Weekly_Sales), breaks = 30)

#Histograms of features
par(mfrow=c(2, 3))
hist(train$Temperature)
hist(train$Fuel_Price)
hist(train$CPI)
hist(train$Unemployment)
plot(table(train$IsHoliday))

# look at some plots
boxplot(Weekly_Sales~Type, data = train, main = "Weekly sales and store types")
boxplot(Weekly_Sales~Temperature>70, data = train, main = "Weekly sales vs temperature > 70 degrees")



```

## STEP 3: EXploratory Data Analysis 
### Perform analysis on training and test data set
```{r echo=TRUE}
summary(train)
glimpse(train)

summary(test)
glimpse(test)

## More analysis on the train dataset 

with(train,table(Dept,Store))   # 45 stores with 99 departments and 143 weeks
with(train,table(Store,Type))  # Determine the type a particular store (3 types)
with(train,table(Dept,Type))   # Determine the type a particular department (3 types)

## Perform the same analysis on the test dataset 
with(test,table(Dept,Store))   # 45 stores with 99 departments and 39 weeks

## Add week number label in the train dataset since we have weekly sales. Train dataset starts from week 5
train$Week_Number <- as.numeric(format(train$Date, "%U"))
test$Week_Number <- as.numeric(format(test$Date, "%U"))

# comparing all departments within each Store which are in 3 types
ggplot(data=train) + 
  geom_bar(mapping=aes(x=Store,colour=Dept))
ggplot(data=train) +
  geom_bar(mapping=aes(x=Store,fill=Dept))+facet_grid(Type ~ .)

# Visualizing a graph that will compare different departments within a selected store
dept_graph <- filter(train, Store==1 & Dept %in% c(1,2,3,4,5,6,7,8,9,10))
ggplot(data=dept_graph) +
  geom_line(mapping=aes(x=Date, y=Weekly_Sales, color=factor(Dept))) + 
  xlab('Date') + ylab('Weekly Sales ($)') + ggtitle('Store 1 Sales bases on multiple departments') +
  scale_colour_discrete(name='Departments')

# Visualizing a graph that will compare same department across different stores
Store_graph <- filter(train, Store %in% c(1,2,3,4,5,6,7,8,9,10) & Dept==1)
ggplot(data=Store_graph) +
  geom_line(mapping=aes(x=Date, y=Weekly_Sales, color=factor(Store))) + 
  xlab('Date') + ylab('Weekly Sales ($)') + ggtitle('Department 1 Sales across multiple Stores') +
scale_colour_discrete(name='Stores')


#  Calculating the sum Total sales across all 45 stores
Sales_Total<-aggregate(train$Weekly_Sales,
                       by=list(Store=train$Store,Type=train$Type,Size=train$Size, Weekly=train$Week_Number),FUN=sum)

sales1<-labs(title="Retail Sales by Stores",y="Sales $")
sales2<-labs(title="Retail Sales by Stores",y="Week number")
ggplot(data=Sales_Total,aes(x=Store,y=x,fill=Type))+geom_bar(stat="identity")+sales1
ggplot(data=Sales_Total,aes(x=Store,y=I(x/Size),fill=Type))+geom_bar(stat="identity")+sales1
ggplot(data=Sales_Total,aes(x=Weekly,y=I(x/Size),fill=Type))+geom_bar(stat="identity")+sales1


# Analysis of the Weekly_Sales trends
# 3 types of stores (aggregate over 45 stores and 99 departments in each store)
All_Sales<-aggregate(train$Weekly_Sales,
                     by=list(Date=train$Date,Type=train$Type),FUN=sum)
# Aggregate the Stores, type and date
Store_Sales<-aggregate(train$Weekly_Sales,
                       by=list(Store=train$Store,Date=train$Date,Type=train$Type),FUN=sum)
# all departments (aggregate over 45 stores)
Dept_Sales<-aggregate(train$Weekly_Sales,
                      by=list(Dept=train$Dept,Date=train$Date,Type=train$Type),FUN=sum)                       

graph0<-labs(title="Store Sales Trend",x="Date",y="Weekly Sales")
graph1<-ggplot(data=All_Sales) + 
  (mapping=aes(x=Date,y=x)) + geom_line(aes(col=Type)) 
graph1+graph0
graph2<-ggplot(data=Store_Sales) + 
  (mapping=aes(x=Date,y=x)) + geom_line(aes(col=Store))
graph2+graph0
graph2+facet_grid(Type ~ .)+graph0
graph3<- ggplot(data=Dept_Sales) + 
  (mapping=aes(x=Date,y=x)) + geom_line(aes(col=Dept))
graph3+graph0

head(train)
tail(train)

fit <- lm(Weekly_Sales ~ as.factor(Type) + Size + CPI + Fuel_Price + IsHoliday + Temperature-1,data=train)
summary(fit)


```

## 4. MODELLING: 
##    The model requires me to predict the weekly sales for each store
```{r echo=TRUE}

### OLS Code has been sampled from online source 
summary(test)
# MarkDown from 1 to 5 turning all NA to 0
train$MarkDown1[is.na(train$MarkDown1)]<-0
train$MarkDown2[is.na(train$MarkDown2)]<-0
train$MarkDown3[is.na(train$MarkDown3)]<-0
train$MarkDown4[is.na(train$MarkDown4)]<-0
train$MarkDown5[is.na(train$MarkDown5)]<-0

# separate data by the store types, 
train_A<-subset(train,Type=="A")
train_B<-subset(train,Type=="B")
train_C<-subset(train,Type=="C")
data_used<-train_C

# using OLS with Store and Dept 
model1<-lm(Weekly_Sales~Size+Store+Dept,data=data_used)
summary(model1)
anova(model1)

model2<-update(model1,.~.+IsHoliday+Temperature+Fuel_Price+CPI+Unemployment)
summary(model2)
anova(model1,model2)

model3<-update(model2,.~.+MarkDown1+MarkDown2+MarkDown3+MarkDown4+MarkDown5)
summary(model3)
anova(model2,model3)

# Ordinary Least Square Estimation with Store and Dept features
m1<-lm(Weekly_Sales~Store+Dept
       +IsHoliday+Temperature+Fuel_Price+CPI+Unemployment
       +MarkDown1+MarkDown2+MarkDown3+MarkDown4+MarkDown5,data=train)
summary(m1)

test$Weekly_Sales<-predict(m1,newdata=test)




```

## 5. Time series Analysis Clustering
```{r echo=TRUE}

# The Code has been obtained from a Kaggle user
#  https://rpubs.com/DharmeshP/CaseStudy1
#
#

# Preparing Train, Test data.


# Sbuseting to prepare Matrix from Train data which is from 2010-02 to 2011-12
train1 <- subset(train, 
                     train$Date > as.Date("2010-02-04",format= "%Y-%m-%d") 
                    & train$Date < as.Date("2012-01-01",format= "%Y-%m-%d"))

# Prepring Regression matrix for AMIMA
regression_matrix   <- data.frame(as.factor(train1$IsHoliday), 
                      train1$Temperature,
                      as.numeric(train1$MarkDown1),as.numeric(train1$MarkDown2),
                      as.numeric(train1$MarkDown3),as.numeric(train1$MarkDown4),
                      as.numeric(train1$Fuel_Price),as.numeric(train1$Unemployment),
                      as.numeric(train1$CPI))


train_matrix <- data.matrix(regression_matrix)

# Checking  TS data
  tsDataSales<-ts(train1$Weekly_Sales,start=c(2010,5),frequency=52)

plot(tsDataSales)

plot(decompose(tsDataSales))

# Preparing Train, Test data.

train_sales<-window(tsDataSales,start=c(2010,5),end=c(2011,52),frequency = 52)

test_sales<-window(tsDataSales,start=2012,frequency = 52)


#Checking for order
Acf(train_sales,lag.max=52,plot=TRUE)

Pacf(train_sales,lag.max=52,plot=TRUE,main="Original Time Series")

# ARIMA with order 1,1,1 gives the best residuals qqnorm plot.
tst_arima <- arima(train_sales)

summary(tst_arima)

#Reshape the train data into a matrix containing the weekly sales for each store
#This is preparation required for time series clustering
#Input: Train dataset which contain multiple rows x 4 column variables
#Output: Matrix of 143 weekly sales observations x 45 stores

store.matrix <- dcast(train,formula=Date~Store,value.var = "Weekly_Sales",fun.aggregate = sum)
store.matrix <- tbl_df(store.matrix)
glimpse(store.matrix)


```