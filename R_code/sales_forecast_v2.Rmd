---
CKME 136: Capstone Fall 2017
Title: Sales_forecast
Author: John-Paul
output: html_document

---
  
##  1. Beginning of the data preprocessing stage. 
##  Setting up a working directory and loading the data into R
```{r echo=TRUE}

# Setting working directory 
setwd('C:/Users/endcore/Desktop/CKME999/CKME136-DataAnalyticsCapstone/Dataset/')

## Load libraries and read the data files
library(tidyverse)
library(reshape2)
library(lubridate)
library(rpart)
library(rattle)
library(forecast)
library(tseries)

## Contains data with the following information
# Store Dept Date Weekly_Sales IsHoliday: 2010-02-05 ~ 2012-10-26
tr_df <- read_csv("train.csv")

## Contains data with the following information
# Store Dept Date IsHoliday: 2012-11-02 ~ 2013-07-26
tst_df <- read_csv("test.csv")

## Contains data with the following information
# Store Type Size
str_df <- read_csv("stores.csv")

## Contains data with the following nformation
# Store Date Temperature Fuel_Price Markdown1-5 CPI Unemplyment IsHoliday
ft_df <- read_csv("features.csv")


```
## 2. DATA PREPARATION FOR ANALYSIS:
  Collecting and preparing the data for analysis are often the most involved and time consuming parts of building a         predictive model. While the collection was done for us, we still have to do a bit of work to prepare the data.
  Merge the data frames "train", "Feature" and store" using the variable "store". 
```{r echo=TRUE}

str(tr_df)
str(ft_df)
head(train)

# Analysis for train dataset and test dataset
str_df$Store <- factor(str_df$Store)
tr_df$Store <- factor(tr_df$Store)
tst_df$Store <- factor(tst_df$Store)
ft_df$Store <- factor(ft_df$Store)
tr_df$Dept <- factor(tr_df$Dept)
tst_df$Dept <- factor(tst_df$Dept)
str_df$Type <- factor(str_df$Type)

## Merge train dataset with stores dataset by store and features dataset 
# training dataset from the following dates: 2010-02-05 ~ 2012-10-26
train1 <- full_join(tr_df, str_df, by= "Store")
train <- merge(x=train1, y=ft_df,all.x=TRUE)

# Export the train datatset

## Merge test dataset with stores dataset by store and features dataset
# testing dataset from the following dates: 2012-11-02 ~ 2013-07-26
test1 <- merge(tst_df, str_df, by="Store")
test <- merge(x=test1, y=ft_df, all.x=TRUE)

rm(tr_df,tst_df,str_df,ft_df, test1, train1)

# Finding which column has NA values
list.NA<-""
for (i in c(1:ncol(train)))
{
  len<-length(grep("TRUE",is.na(train[,i])))
  if(len > 0){
    list.NA<-paste(colnames(train[i]),":",len,list.NA)
  }
}
list.NA
str(train)
sum(is.na(train))

levels(train$Type) # 3 retail store types: "A", "B", and "C"
levels(train$Store)

## We can then use the complete.cases function to identify the rows without missing data:

#train1 <- train[complete.cases(train),]
#test1 <- test[complete.cases(test),]

summary(train)

ggplot(train1, aes(Date,Weekly_Sales)) + geom_line() + scale_x_date('month') + 
  ylab("Weekly_Sales recording") + xlab("")


glimpse(train)
dim(train)

glimpse(test)
dim(test)

```

## 3. EXploratory Data Analysis 
### Add week number label in the train dataset since we have weekly sales. Train dataset starts from week 5
```{r echo=TRUE}
## More analysis on the train dataset 
with(train,table(Dept,Store))  # 45 stores with 99 departments
with(train,table(Store,Type))  # 3 types (Store and Type are exclusive)
with(train,table(Dept,Type))

# 99 Depts within 45 Stores which are in 3 types
ggplot(data=train) + 
  geom_bar(mapping=aes(x=Store,colour=Dept))
ggplot(data=train) +
  geom_bar(mapping=aes(x=Store,fill=Dept))+facet_grid(Type ~ .)


## Add week number label in the train dataset since we have weekly sales. Train dataset starts from week 5
train$Week_Number <- as.numeric(format(train$Date, "%U"))
test$Week_Number <- as.numeric(format(test$Date, "%U"))

# Total sales across 45 stores
Sales_Total<-aggregate(train$Weekly_Sales,
                       by=list(Store=train$Store,Type=train$Type,Size=train$Size, Weekly=train$Week_Number),FUN=sum)

sales1<-labs(title="retail Sales by Stores",y="Sales $")
sales2<-labs(title="retail Sales by Stores",y="Week number")
ggplot(data=Sales_Total,aes(x=Store,y=x,fill=Type))+geom_bar(stat="identity")+sales1
ggplot(data=Sales_Total,aes(x=Store,y=I(x/Size),fill=Type))+geom_bar(stat="identity")+sales1
ggplot(data=Sales_Total,aes(x=Weekly,y=I(x/Size),fill=Type))+geom_bar(stat="identity")+sales1


# Analysis of the Weekly_Sales trends
# 3 types of stores (aggregate over 45 stores and 99 departments in each store)
All_Sales<-aggregate(train$Weekly_Sales,
                     by=list(Date=train$Date,Type=train$Type),FUN=sum)
# Aggregate the Stores, type and date
Store_Sales<-aggregate(train$Weekly_Sales,
                       by=list(Store=train$Store,Date=train$Date,Type=train$Type),FUN=sum)
# all departments (aggregate over 45 stores)
Dept_Sales<-aggregate(train$Weekly_Sales,
                      by=list(Dept=train$Dept,Date=train$Date,Type=train$Type),FUN=sum)                       

graph0<-labs(title="Retail Sales Trend",x="Date",y="Weekly Sales")
graph1<-ggplot(data=All_Sales) + 
  (mapping=aes(x=Date,y=x)) + geom_line(aes(col=Type)) 
graph1+graph0
graph2<-ggplot(data=Store_Sales) + 
  (mapping=aes(x=Date,y=x)) + geom_line(aes(col=Store))
graph2+graph0
graph2+facet_grid(Type ~ .)+graph0


#head(train)


## Add a holiday label to the train and test dataset
#train$Holiday <- as.character(c("Super bowl", "Labour day", "Thanks giving", "Christmas"))
#train$Holiday <- data.frame(train, value=c("Super bowl", "Labour day", "Thanks giving", "Christmas"))
#glimpse(train)

## Weekly_sales has some negative numbers which indicates goods returned to store after sale
#train$Returns <- lapply(train$Weekly_Sales,function(sales){
#  ifelse(sales < 0,sales,0)
#})
#train$Weekly_Sales <- lapply(train$Weekly_Sales,function(sales){
#  ifelse(sales > 0,sales,0)
#})

#tail(train)



```

## 4. Regression Analysis: 
##    The model requires as to predict the weekly sales for each store
```{r echo=TRUE}

# MarkDown from 1 to 5 are probably the deals 
train$MarkDown1[is.na(train$MarkDown1)]<-0
train$MarkDown2[is.na(train$MarkDown2)]<-0
train$MarkDown3[is.na(train$MarkDown3)]<-0
train$MarkDown4[is.na(train$MarkDown4)]<-0
train$MarkDown5[is.na(train$MarkDown5)]<-0

# separate data by the store types, select one to use for OLS
train_A<-subset(train,Type=="A")
train_B<-subset(train,Type=="B")
train_C<-subset(train,Type=="C")
data_used<-train_C

# using OLS with Store and Dept dummyies
# equivalent to panel data fixed effects
model1<-lm(Weekly_Sales~Size+Store+Dept,data=data_used)
summary(model1)
anova(model1)

model2<-update(model1,.~.+IsHoliday+Temperature+Fuel_Price+CPI+Unemployment)
summary(model2)
anova(model1,model2)

model3<-update(model2,.~.+MarkDown1+MarkDown2+MarkDown3+MarkDown4+MarkDown5)
summary(model3)
anova(model2,model3)




```

## 5. Time series Analysis Clustering
```{r echo=TRUE}


#Reshape the train data into a matrix containing the weekly sales for each store
#This is preparation required for time series clustering
#Input: Train dataset which contain multiple rows x 4 column variables
#Output: Matrix of 143 weekly sales observations x 45 stores
store.matrix <- dcast(train,formula=Date~Store,value.var = "Weekly_Sales",fun.aggregate = sum)
store.matrix <- tbl_df(store.matrix)
glimpse(store.matrix)

#which store and type of store have the most sales
## Add all department sales per store to get the total weekly sales
aggregate(train[,"Weekly_Sales"], by=train[,c("Store"), drop=FALSE], sum)
aggregate(train[,"Weekly_Sales"], by=train[,c("Store"), drop=FALSE], mean)
aggregate(train[,"Weekly_Sales"], by=train[,c("Store"), drop=FALSE], max)


```