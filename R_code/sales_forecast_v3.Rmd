---
Author: John-Paul
CKME 136: Capstone Fall 2017
Title: Sales_forecast
output:
  html_document: default
  pdf_document: default
  word_document: default
---
  
##  Step 1. LOADING THE DATA
##  Beginning of the data preprocessing stage. 
##  Setting up a working directory and loading the data and libraries into R
```{r echo=TRUE}

# Setting working directory 
setwd('C:/Users/endcore/Desktop/CKME999/CKME136-DataAnalyticsCapstone/Dataset/')

## Load libraries and read the data files
library(tidyverse)
library(reshape2)
library(lubridate)
library(rpart)
library(rattle)
library(forecast)
library(tseries)
library(dygraphs)
library(fpp)
library(pastecs)
library(caret)
library(corrplot)

## Contains data with the following information
# Store Dept Date Weekly_Sales IsHoliday: 2010-02-05 ~ 2012-10-26
tr_df <- read_csv("train.csv")

## Contains data with the following information
# Store Dept Date IsHoliday: 2012-11-02 ~ 2013-07-26
tst_df <- read_csv("test.csv")

## Contains data with the following information
# Store Type Size
str_df <- read_csv("stores.csv")

## Contains data with the following nformation
# Store Date Temperature Fuel_Price Markdown1-5 CPI Unemplyment IsHoliday
ft_df <- read_csv("features.csv")


```
## Step 2: DATA PREPARATION FOR ANALYSIS
#  Collecting and preparing the data for analysis are often the most involved and time consuming parts of building a         predictive model. While the collection was done for us, we still have to do a bit of work to prepare the data.
#  Merge the data frames "train", "Feature" and store" using the variable "store". 
```{r echo=TRUE}

str(tr_df)
str(ft_df)
str(tst_df)
str(str_df)

# Analysis for train dataset and test dataset 
# Converting the  Store, Dept and Type features to factor datatype
str_df$Store <- factor(str_df$Store)
tr_df$Store <- factor(tr_df$Store)
tst_df$Store <- factor(tst_df$Store)
ft_df$Store <- factor(ft_df$Store)
tr_df$Dept <- factor(tr_df$Dept)
tst_df$Dept <- factor(tst_df$Dept)
str_df$Type <- factor(str_df$Type)

head(tr_df, n=3)
head(tst_df, n=3)

```
# Now In this step of the process, we need to merge the datasets to build a successive model. We first review the column  # from Store.csv and join it with train.csv datasets. Then with the new dataset we do another join operation with #feature.csv dataset.
```{r echo=TRUE}
## Merge train dataset with stores dataset by store and features dataset 
# training dataset from the following dates: 2010-02-05 ~ 2012-10-26
train1 <- full_join(tr_df, str_df, by= "Store")
train <- merge(x=train1, y=ft_df, by=c("Store", "Date", "IsHoliday"), all.x=TRUE, sort=FALSE)

# Export the train datatset
#Create a csv file with the merged dataset train
 
#write.csv(train, file = "C:/Users/endcore/Documents/Capstone/train_merged.csv",row.names=FALSE, na="")

## Merge test dataset with stores dataset by store, Date and IsHoliday features
# testing dataset from the following dates: 2012-11-02 ~ 2013-07-26
test1 <- merge(tst_df, str_df, by="Store", sort = FALSE)
test <- merge(x=test1, y=ft_df, by=c("Store", "Date", "IsHoliday"), all.x=TRUE)
test <- arrange(test, Store, Dept)

rm(tr_df,tst_df,str_df,ft_df, test1, train1)

head(train, n=5)
head(test, n=5)



```
# Performing more Statistical analysis and looking for missing data
```{r echo=TRUE}

colSums(is.na(train))   # Number of missing per column/variable
colSums(is.na(test))   # Number of missing per column/variable


# Finding which column has NA values
list.NA<-""
for (i in c(1:ncol(train)))
{
  len<-length(grep("TRUE",is.na(train[,i])))
  if(len > 0){
    list.NA<-paste(colnames(train[i]),":",len,list.NA)
  }
}
glimpse(list.NA)
#str(train)

# Number of missing values
sum(is.na(train))

## We can then use the complete.cases function to identify the rows without missing data:
# In train dataset has 421,570 variables and 324,514 have missing values
# Complete cases in train dataset are 97,056 and renamed to train_clean
#  

train_clean <- train[complete.cases(train),]
test1 <- test[complete.cases(test),]
#glimpse(train)
#stat.desc(train_clean)
#summary(train_clean)

train[is.na(train)] <- 0

glimpse(train)
#glimpse(train_clean)
#train1


```

```{r echo=TRUE}
levels(train_clean$Type) # 3 retail store types: "A", "B", and "C"
levels(train_clean$Store)

hist(log(train$Weekly_Sales), breaks = 30)

#Histograms of features
par(mfrow=c(2, 3))
with(train_clean, hist(Temperature))
with(train_clean, hist(Fuel_Price))
with(train_clean, hist(CPI))
with(train_clean, hist(Unemployment))

# look at some plots
boxplot(Weekly_Sales~Type, data = train_clean, main = "Weekly sales and store types")
boxplot(Weekly_Sales~Temperature>70, data = train_clean, main = "Weekly sales vs temperature > 70 degrees")



```
# Add week number label in the train dataset since we have weekly sales. Train dataset starts from week 5
```{r echo=TRUE}
## Add week number label in the train dataset since we have weekly sales. Train dataset starts from week 5
train_clean$Week_Number <- as.numeric(format(train_clean$Date, "%U"))
test$Week_Number <- as.numeric(format(test$Date, "%U"))
glimpse(train_clean)

```
 # In order for our analysis to be consistent and avoid testing on our training data, we are going to split it into a
 # training and test data set using the caret package
 # The training data set has 77,648
 # The testing data set has 19,408
```{r echo=TRUE}
#Subset our data into train and test
index <- createDataPartition(train_clean$Weekly_Sales,list = FALSE,p=0.8)
training <-train_clean[index,]
testing <- train_clean[-index,]

nrow(training)
nrow(testing)


```

## STEP 3: EXploratory Data Analysis 
### Perform analysis on training and testing data set
```{r echo=TRUE}


## More analysis on the training dataset 

with(training,table(Dept,Store))   # 45 stores with 99 departments and result is the number of weeks
with(training,table(Store,Type))  # Determine the type a particular store (3 types)
with(training,table(Dept,Type))   # Determine the type a particular department (3 types)

## Perform the same analysis on the test dataset 
with(test,table(Dept,Store))   # 45 stores with 99 departments and 39 weeks

# comparing all departments within each Store which are in 3 types
ggplot(data=training) + 
  geom_bar(mapping=aes(x=Store,colour=Dept))
ggplot(data=training) +
  geom_bar(mapping=aes(x=Store,fill=Dept))+facet_grid(Type ~ .)

# Visualizing a graph that will compare different departments within a selected store
dept_graph <- filter(training, Store==1 & Dept %in% c(1,2,3,4,5,6,7,8,9,10))
ggplot(data=dept_graph) +
  geom_line(mapping=aes(x=Date, y=Weekly_Sales, color=factor(Dept))) + 
  xlab('Date') + ylab('Weekly Sales ($)') + ggtitle('Store 1 Sales bases on multiple departments') +
  scale_colour_discrete(name='Departments')

# Visualizing a graph that will compare same department across different stores
Store_graph <- filter(training, Store %in% c(1,2,3,4,5,6,7,8,9,10) & Dept==1)
ggplot(data=Store_graph) +
  geom_line(mapping=aes(x=Date, y=Weekly_Sales, color=factor(Store))) + 
  xlab('Date') + ylab('Weekly Sales ($)') + ggtitle('Department 1 Sales across multiple Stores') +
scale_colour_discrete(name='Stores')


#  Calculating the sum Total sales across all 45 stores
Sales_Total<-aggregate(training$Weekly_Sales,
                       by=list(Store=training$Store,Type=training$Type,Size=training$Size, Weekly=training$Week_Number),FUN=sum)

sales1<-labs(title="Retail Sales by Stores",y="Sales $")
sales2<-labs(title="Retail Sales by Stores",y="Week number")
ggplot(data=Sales_Total,aes(x=Store,y=x,fill=Type))+geom_bar(stat="identity")+sales1
ggplot(data=Sales_Total,aes(x=Store,y=I(x/Size),fill=Type))+geom_bar(stat="identity")+sales1
ggplot(data=Sales_Total,aes(x=Weekly,y=I(x/Size),fill=Type))+geom_bar(stat="identity")+sales1


# Analysis of the Weekly_Sales trends
# 3 types of stores (aggregate over 45 stores and 99 departments in each store)
All_Sales<-aggregate(training$Weekly_Sales,
                     by=list(Date=training$Date,Type=training$Type),FUN=sum)
# Aggregate the Stores, type and date
Store_Sales<-aggregate(training$Weekly_Sales,
                       by=list(Store=training$Store,Date=training$Date,Type=training$Type),FUN=sum)
# all departments (aggregate over 45 stores)
Dept_Sales<-aggregate(training$Weekly_Sales,
                      by=list(Dept=training$Dept,Date=training$Date,Type=training$Type),FUN=sum)                       

graph0<-labs(title="Store Sales Trend",x="Date",y="Weekly Sales")
graph1<-ggplot(data=All_Sales) + 
  (mapping=aes(x=Date,y=x)) + geom_line(aes(col=Type)) 
graph1+graph0
graph2<-ggplot(data=Store_Sales) + 
  (mapping=aes(x=Date,y=x)) + geom_line(aes(col=Store))
graph2+graph0
graph2+facet_grid(Type ~ .)+graph0
graph3<- ggplot(data=Dept_Sales) + 
  (mapping=aes(x=Date,y=x)) + geom_line(aes(col=Dept))
graph3+graph0

head(training)
tail(training)


```
## 4. MODELLING: 
##  Ordinary Least Square Linear regression modelling  
# The model requires me to predict the weekly sales for each store
```{r echo=TRUE}
fit <- lm(Weekly_Sales ~ Date + as.factor(Type) + Size + CPI + Fuel_Price + IsHoliday + Temperature-1,data=train)
fitconf_predict <- predict(fit, newdata=test, interval="confidence", level=0.95)


fit_predict <-predict(fit, newdata=test, interval="prediction", level=0.95)

predict_unc <- cbind(test,fitconf_predict,fit_predict[,-1])
test <- cbind(test,fitconf_predict)
results <- ifelse(test$Weekly_Sales >= test$lwr & test$Weekly_Sales <= test$upr,TRUE,FALSE)
#test <- cbind(test,results)
sum(results)/nrow(test)

# separate data by the store types, 
train_A<-subset(train,Type=="A")
train_B<-subset(train,Type=="B")
train_C<-subset(train,Type=="C")
data_used<-train_C

# using OLS with Store and Dept 
model1<-lm(Weekly_Sales~Size+Store+Dept,data=data_used)
summary(model1)
anova(model1)

model2<-update(model1,.~.+IsHoliday+Temperature+Fuel_Price+CPI+Unemployment)
summary(model2)
anova(model1,model2)

model3<-update(model2,.~.+MarkDown1+MarkDown2+MarkDown3+MarkDown4+MarkDown5)
summary(model3)
anova(model2,model3)

# Ordinary Least Square Estimation with Store and Dept features
m1<-lm(Weekly_Sales~Store+Dept
       +IsHoliday+Temperature+Fuel_Price+CPI+Unemployment
       +MarkDown1+MarkDown2+MarkDown3+MarkDown4+MarkDown5,data=train)
summary(m1)

test$Weekly_Sales<-predict(m1,newdata=test)




```

## 5. Time series Analysis Clustering
```{r echo=TRUE}

# Some Code has been sampled from a Kaggle user
#  https://rpubs.com/DharmeshP/CaseStudy1
#
#
# Preparing Train, Test data.
# Preparing a time series for the train dataset
head(train, n=3)

train_s <- count(training, c("Date"))
train_sales_ts <- ts(training$Weekly_Sales, start = c(2010,5),
               end = c(2012,52), frequency = 52)

# The following plot shows our time series
plot(train_sales_ts)

# Analyzing the train dataset using the ARIMA model
d <- 0 : 2
p <- 0 : 6
q <- 0 : 6

train_models <- expand.grid(d = d, p = p, q = q)
head(train_models, n = 4)

# define ARIMA function that fits a model
getTSModelAIC <- function(ts_data, p, d, q) {
ts_model <- arima(ts_data, order = c(p, d, q))
return(ts_model$aic)
}

getTSModelAICSafe <- function(ts_data, p, d, q) {
result = tryCatch({
  getTSModelAIC(ts_data, p, d, q)
}, error = function(e) {
Inf
})
}

train_models$aic <- mapply(function(x, y, z)
getTSModelAICSafe(train_sales_ts, x, y, z), train_models$p,
train_models$d, train_models$q)
subset(train_models,aic == min(aic))

# The results indicate that the most appropriate model for our Weekly sales
# time series is the ARIMA(2, 1, 3) model. We can train this model again with these parameters:
train_model <- arima(train_sales_ts, order = c(2, 1, 3))
summary(train_model)

#Plot and forecast the next future points
plot(forecast(train_model))

```